{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb51644",
   "metadata": {},
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db7ddd54",
   "metadata": {},
   "source": [
    "Lasso Regression, also known as L1 regularization, is a linear regression technique used in machine learning and statistics for both feature selection and regularization. It is a variant of linear regression that introduces a penalty term to the standard linear regression cost function, making it particularly useful when dealing with high-dimensional datasets where the number of features (variables) is much greater than the number of observations (data points).\n",
    "\n",
    "Here's how Lasso Regression differs from other regression techniques, particularly from the more traditional linear regression:\n",
    "\n",
    "1. L1 Regularization: Lasso Regression adds a penalty term to the linear regression cost function, which is proportional to the absolute values of the regression coefficients (the weights assigned to each feature). This encourages the model to reduce the magnitude of some coefficients to zero, effectively performing feature selection by excluding irrelevant features. In contrast, traditional linear regression doesn't introduce such a penalty term and will use all features in the model.\n",
    "\n",
    "2. Feature Selection: Lasso Regression can automatically perform feature selection by driving some coefficients to zero. This is very useful when you have a large number of features and want to identify the most important ones. In contrast, traditional linear regression uses all available features and doesn't inherently select a subset of them.\n",
    "\n",
    "3. Sparsity: Lasso tends to produce sparse models, meaning it results in models with fewer non-zero coefficients. This can make the model more interpretable and reduce the risk of overfitting, especially when dealing with high-dimensional data.\n",
    "\n",
    "4. Bias-Variance Trade-off: Like Ridge Regression (L2 regularization), Lasso introduces a bias in the parameter estimates to reduce overfitting. However, Lasso's use of the L1 penalty has a stronger biasing effect compared to Ridge's L2 penalty, and it tends to perform better in situations where there are a few important features and many irrelevant ones.\n",
    "\n",
    "5. Loss Function: Lasso minimizes a modified loss function that includes the sum of absolute values of the regression coefficients, whereas traditional linear regression minimizes the mean squared error.\n",
    "\n",
    "6. Variable Shrinkage: Lasso tends to shrink the coefficients of less important features towards zero, effectively eliminating them from the model. This results in a simpler model with potentially better generalization to new data.\n",
    "\n",
    "7. Optimization: Lasso Regression typically requires specialized optimization techniques such as coordinate descent or subgradient methods because of the non-differentiable nature of the L1 penalty term.\n",
    "\n",
    "\n",
    "Lasso Regression is a valuable technique when you want to perform feature selection, reduce overfitting, and create a simpler, more interpretable model in situations with high-dimensional data. It differs from traditional linear regression and other regression techniques like Ridge Regression, which use different regularization methods and have different properties when it comes to feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59733cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f461eca",
   "metadata": {},
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1afac1",
   "metadata": {},
   "source": [
    "## The main advantage of using Lasso Regression in feature selection is its ability to automatically select features by shrinking the coefficients of irrelevant features to zero. This can simplify the model and make it more interpretable. Other regression techniques, such as linear regression, do not perform feature selection and can be difficult to interpret on high-dimensional datasets.\n",
    "\n",
    "\n",
    "\n",
    "## `Lasso regression also has several other advantages for feature selection, including:`\n",
    "\n",
    "`It can handle correlated features. Lasso regression is able to select features even when they are correlated, which is a common problem in high-dimensional datasets. Other regression techniques, such as linear regression, can have difficulty selecting correlated features.`\n",
    "\n",
    "`It is robust to outliers. Lasso regression is less sensitive to outliers than other regression techniques, such as linear regression. This means that it is less likely to be affected by noisy or incomplete data.`\n",
    "\n",
    "\n",
    "`It is computationally efficient. Lasso regression can be trained efficiently on large datasets. This makes it a practical solution for feature selection on high-dimensional data.`\n",
    "\n",
    "\n",
    "## `Overall, Lasso regression is a powerful tool for feature selection. It can automatically select features, handle correlated features, and is robust to outliers. It is also computationally efficient, making it a practical solution for feature selection on large datasets`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## `Here are some examples of how Lasso regression can be used for feature selection:`\n",
    "\n",
    "`A company might use Lasso regression to select features from a customer database to predict customer churn.`\n",
    "\n",
    "\n",
    "`A bank might use Lasso regression to select features from a financial database to detect fraud.`\n",
    "\n",
    "\n",
    "`A medical researcher might use Lasso regression to select features from a medical database to predict the risk of a disease.`\n",
    "\n",
    "\n",
    "`In all of these cases, Lasso regression can be used to identify the most important features for predicting the target`\n",
    "\n",
    "\n",
    "\n",
    "`variable. This can help to build more accurate and interpretable models.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcff11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2a1572",
   "metadata": {},
   "source": [
    "# `Q3. How do you interpret the coefficients of a Lasso Regression model?`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d9d8ae7",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting coefficients in linear regression, but there are some differences due to the L1 regularization used in Lasso. Here's how you can interpret the coefficients in a Lasso Regression model:\n",
    "\n",
    "1. Coefficient Sign: Like in linear regression, the sign of a coefficient in a Lasso model indicates whether the feature has a positive or negative effect on the target variable. If the coefficient is positive, it means that an increase in the feature's value is associated with an increase in the predicted target value, and if it's negative, it means the opposite.\n",
    "\n",
    "2. Magnitude of Coefficients: The magnitude (absolute value) of a coefficient represents the strength of the relationship between the feature and the target variable. Larger absolute values suggest stronger influences. In Lasso Regression, some coefficients may be exactly zero, which means the corresponding features are excluded from the model. This is a form of feature selection. Non-zero coefficients indicate that the associated features are important in making predictions.\n",
    "\n",
    "3. Sparsity: Lasso Regression has a tendency to produce sparse models, meaning it sets some coefficients to exactly zero. This can be interpreted as a form of automatic feature selection. Features with non-zero coefficients are considered important in predicting the target variable, while features with zero coefficients are deemed unimportant.\n",
    "\n",
    "4. Feature Importance: By examining the non-zero coefficients, you can assess the relative importance of different features in the Lasso model. Features with larger non-zero coefficients have a greater impact on the predictions, while features with smaller non-zero coefficients have a relatively smaller influence.\n",
    "\n",
    "5. Regularization Strength: The magnitude of the L1 penalty term in the Lasso model determines the extent to which coefficients are shrunk towards zero. A larger penalty (higher regularization strength) will result in more coefficients being exactly zero, leading to a simpler model with fewer features.\n",
    "\n",
    "6. Direction of Influence: The sign and magnitude of the coefficients indicate not only the presence and strength of the relationship but also the direction. For example, if the coefficient of a feature is positive, it suggests that increasing that feature value positively impacts the target variable, and vice versa for negative coefficients.\n",
    "\n",
    "7. Be Cautious with Multicollinearity: In Lasso Regression, multicollinearity (high correlations between features) can affect coefficient interpretation. Lasso may select one of the correlated features while driving others to zero. In such cases, the interpretation of individual feature coefficients may be less straightforward.\n",
    "\n",
    "To interpret the coefficients effectively, it's essential to have a good understanding of the context of your specific regression problem, the domain, and the meaning of the features. Additionally, visualizations and statistical tests can help gain insights into the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377220b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cea77289",
   "metadata": {},
   "source": [
    "# `Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3eb9006",
   "metadata": {},
   "source": [
    "In Lasso Regression, there is typically one primary tuning parameter to adjust, which controls the strength of the L1 regularization penalty. This tuning parameter is often denoted as \"alpha\" and is sometimes referred to as the regularization strength. The value of alpha determines the degree of regularization applied to the model, and it can significantly affect the model's performance. Here's how it works:\n",
    "\n",
    "1. Alpha: The primary tuning parameter in Lasso Regression, alpha, is a positive scalar value that controls the balance between fitting the data and preventing overfitting through L1 regularization. When alpha is set to 0, the L1 penalty has no effect, and Lasso Regression reduces to standard linear regression. As alpha increases, the regularization strength becomes stronger, and the model becomes more constrained.\n",
    "\n",
    "Small α: When alpha is small, the model places less emphasis on regularization, and it is more likely to select a larger number of features and assign non-zero coefficients to many features. This can result in a model that captures noise in the data and is prone to overfitting.\n",
    "\n",
    "Moderate α: Choosing a moderate value of alpha strikes a balance between fitting the data and regularizing the model. This is often a good starting point when tuning the hyperparameter, as it can lead to a model that includes important features while reducing overfitting.\n",
    "\n",
    "Large α: A large alpha encourages stronger regularization, and it is more likely to set coefficients to exactly zero, leading to feature selection. This simplifies the model, reduces overfitting, and makes it more interpretable.\n",
    "\n",
    "To find the optimal alpha for your Lasso Regression model, you can use techniques like cross-validation. Cross-validation involves training the model with different values of alpha and evaluating its performance on multiple subsets of the training data. By choosing the alpha that results in the best performance on the validation data, you can strike a balance between model complexity and predictive power.\n",
    "\n",
    "It's worth noting that there are libraries and tools in popular programming languages like Python (e.g., scikit-learn) that provide built-in functions for performing hyperparameter tuning, such as GridSearchCV or RandomizedSearchCV, which can automate the process of finding the best alpha value for your Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b9749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a79deb6",
   "metadata": {},
   "source": [
    "# `Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ba2c873",
   "metadata": {},
   "source": [
    "Yes, Lasso regression can be used for non-linear regression problems by using a basis function expansion. A basis function expansion is a technique for transforming a non-linear function into a linear combination of basis functions. Basis functions are typically simple functions, such as polynomials, splines, or wavelets.\n",
    "\n",
    "Lasso Regression, in its standard form, is a linear regression technique designed for linear relationships between features and the target variable. It's particularly useful when you want to perform feature selection and apply L1 regularization to linear models. However, it's not suitable for addressing non-linear regression problems on its own because it doesn't model non-linear relationships.\n",
    "\n",
    "Once the non-linear function has been transformed into a linear combination of basis functions, Lasso regression can be applied to the transformed function. Lasso regression will then select the most important basis functions and shrink the coefficients of the less important basis functions to zero. This will result in a sparse model that can accurately approximate the non-linear function.\n",
    "\n",
    "Here are some of the advantages of using Lasso regression for non-linear regression problems:\n",
    "\n",
    "Improved accuracy: Lasso regression can improve the accuracy of non-linear regression models by reducing overfitting and selecting the most important features.\n",
    "\n",
    "Interpretability: Lasso regression can produce more interpretable non-linear regression models by shrinking the coefficients of the less important features to zero.\n",
    "\n",
    "Computational efficiency: Lasso regression can be trained efficiently on large datasets, making it a practical solution for non-linear regression on high-dimensional data.\n",
    "\n",
    "\n",
    "Here are some examples of how Lasso regression can be used for non-linear regression problems:\n",
    "\n",
    "Predicting house prices based on non-linear features such as square footage, number of bedrooms, and location\n",
    "\n",
    "Predicting customer churn based on non-linear features such as customer demographics, usage history, and satisfaction levels\n",
    "\n",
    "Predicting medical risk based on non-linear features such as age, family history, and lifestyle\n",
    "\n",
    "Detecting fraud based on non-linear features such as transaction history and account information\n",
    "\n",
    "\n",
    "\n",
    "Lasso regression is a powerful tool for non-linear regression. It can produce accurate and interpretable models that are not prone to overfitting. It is also computationally efficient, making it a practical solution for non-linear regression on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22e58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fdf7638",
   "metadata": {},
   "source": [
    "# `Q6. What is the difference between Ridge Regression and Lasso Regression?`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61e8ff47",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are two popular techniques for linear regression that both involve regularization to improve the performance of models, especially when dealing with high-dimensional datasets. While they have a similar goal of reducing overfitting, they use different types of regularization, leading to some key differences:\n",
    "\n",
    "1. Regularization Type:\n",
    "\n",
    "Ridge Regression: Ridge Regression, also known as L2 regularization, adds a penalty term to the linear regression cost function that is proportional to the sum of the squared values of the regression coefficients. This is expressed as α∑(βi^2), where α is the regularization strength, and βi represents the coefficients.\n",
    "\n",
    "Lasso Regression: Lasso Regression, also known as L1 regularization, introduces a penalty term based on the sum of the absolute values of the regression coefficients. The penalty term is expressed as α∑|βi|, where α is the regularization strength, and βi represents the coefficients.\n",
    "\n",
    "2.Penalty Effect:\n",
    "\n",
    "Ridge Regression: Ridge Regression tends to shrink the coefficients of all features toward zero but rarely sets them exactly to zero. It reduces the magnitude of the coefficients, effectively preventing any single feature from dominating the model.\n",
    "\n",
    "Lasso Regression: Lasso Regression has a stronger tendency to set some coefficients to exactly zero. This results in automatic feature selection, meaning it can exclude irrelevant features from the model, effectively simplifying the model.\n",
    "\n",
    "3. Sparsity:\n",
    "\n",
    "Ridge Regression: Ridge Regression typically does not produce sparse models, meaning all features are used in the model. The coefficients are reduced in magnitude, but they remain non-zero.\n",
    "\n",
    "Lasso Regression: Lasso Regression often produces sparse models, where some features are assigned exactly zero coefficients. This results in feature selection, which is valuable when dealing with high-dimensional datasets.\n",
    "\n",
    "4. Model Complexity:\n",
    "\n",
    "Ridge Regression: Ridge Regression generally retains all features in the model but reduces the magnitude of their coefficients. It can be more appropriate when you believe that all features are relevant but want to reduce the risk of overfitting.\n",
    "\n",
    "Lasso Regression: Lasso Regression can simplify the model by selecting a subset of features while reducing overfitting. This is useful when you suspect that only a subset of features is relevant to the target variable.\n",
    "\n",
    "5. Interpretability:\n",
    "\n",
    " Ridge Regression: Ridge Regression is typically less straightforward for feature selection and interpretation because it retains all features, albeit with reduced coefficients.\n",
    "\n",
    "Lasso Regression: Lasso Regression offers better interpretability due to its ability to set some coefficients to exactly zero, effectively identifying the most important features.........................\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c37fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "859f6380",
   "metadata": {},
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4149a049",
   "metadata": {},
   "source": [
    "Lasso Regression can help mitigate multicollinearity to some extent, but it does not handle multicollinearity in the same way as Ridge Regression or other techniques designed specifically for addressing multicollinearity. Multicollinearity occurs when two or more input features are highly correlated, which can lead to instability in the coefficient estimates. Lasso's primary focus is on feature selection and regularization, but it can indirectly have an impact on multicollinearity. Here's how Lasso Regression interacts with multicollinearity:\n",
    "\n",
    "1. Feature Selection: Lasso Regression has a feature selection property that tends to set some coefficients to exactly zero. When it encounters multicollinearity, it may preferentially select one of the correlated features while driving the others to zero. This can effectively address multicollinearity by excluding some of the correlated features from the model.\n",
    "\n",
    "2. Reducing Model Complexity: By setting some coefficients to zero, Lasso simplifies the model. This simplification can be advantageous when dealing with multicollinearity because it reduces the complexity of the model, potentially making it more stable and interpretable.\n",
    "\n",
    "3. Bias in Coefficient Estimates: While Lasso's feature selection property can help with multicollinearity, it's essential to note that it can introduce bias into the coefficient estimates. The selected features might not necessarily represent the \"true\" relationships in the data, and the coefficients of the selected features might be biased due to the presence of multicollinearity.\n",
    "\n",
    "4. Limitations: Lasso's ability to address multicollinearity is limited to the extent that it selects and retains the most important features while driving others to zero. However, the strength of multicollinearity and the amount of regularization (determined by the alpha parameter) can influence the effectiveness of Lasso in handling multicollinearity. In cases of very high multicollinearity, Lasso may not completely resolve the issue, and other techniques like Ridge Regression or Principal Component Analysis (PCA) might be more effective.\n",
    "\n",
    "To more effectively address multicollinearity, we may consider the following approaches:\n",
    "\n",
    "1. Ridge Regression: Ridge Regression, with its L2 regularization, is generally more effective at reducing the magnitude of coefficients and stabilizing them in the presence of multicollinearity. It does not set coefficients to zero but rather shrinks them toward zero.\n",
    "\n",
    "2. Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can decorrelate the input features and transform them into a set of orthogonal variables. This can help mitigate multicollinearity.\n",
    "\n",
    "3. Feature Engineering: You can manually create new features or linear combinations of existing features to reduce multicollinearity. For example, you can create interaction terms or use domain knowledge to create more meaningful features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2eb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca42ac1",
   "metadata": {},
   "source": [
    "# `Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65ca42c3",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (often denoted as \"lambda\" or \"alpha\") in Lasso Regression is a critical step in building an effective model. The goal is to strike a balance between model simplicity (high regularization, smaller coefficients) and model accuracy (low regularization, larger coefficients). Here are some common approaches to choose the optimal lambda in Lasso Regression:\n",
    "\n",
    "1. Cross-Validation:\n",
    "\n",
    "   Cross-validation is one of the most widely used methods for selecting the optimal lambda in Lasso Regression. The basic idea is to train and evaluate the model with different values of lambda on various subsets of wer data. Common cross-validation techniques include k-fold cross-validation or leave-one-out cross-validation (LOOCV).\n",
    "\n",
    " For each value of lambda, fit the Lasso Regression model on the training data and evaluate its performance on the validation or test data.\n",
    " Repeatedly perform this process for different lambda values.\n",
    " \n",
    " Choose the lambda that results in the best model performance on the validation data (e.g., lowest mean squared error for regression tasks or other appropriate evaluation metrics).\n",
    " \n",
    " \n",
    "\n",
    " In Python, libraries like scikit-learn provide tools like `GridSearchCV` and `LassoCV` that can automate the cross-validation process for lambda selection.\n",
    "\n",
    "2. Information Criteria:\n",
    "\n",
    "   Information criteria such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can be used to select the optimal lambda. These criteria balance model fit and model complexity, providing a quantitative measure of how well the model explains the data while penalizing model complexity.\n",
    "\n",
    " Fit Lasso Regression models with different lambda values.\n",
    "Compute the AIC or BIC for each model.\n",
    "  Select the lambda that minimizes the AIC or BIC, as this value represents the best trade-off between model fit and complexity.\n",
    "\n",
    "3. Regularization Path:\n",
    "\n",
    "   we can calculate the entire regularization path of the Lasso Regression, which includes the coefficients for various lambda values. This approach allows we to visualize how the coefficients change with different lambda values. The optimal lambda can be chosen based on the pattern of coefficient changes. A common plot used for this purpose is the \"Lasso path\" or \"Lasso coefficient trajectory.\"\n",
    "\n",
    "  Fit Lasso Regression models for a range of lambda values.\n",
    "   Plot the coefficients against the log-scale lambda values.\n",
    "   Observe the point at which some coefficients become exactly zero, and select the corresponding lambda as the optimal value.\n",
    "\n",
    "4. Stable Variables:\n",
    "\n",
    "   we can monitor the stability of the selected variables across different lambda values. Variables that consistently remain in the model over a range of lambda values are likely to be more stable and important.\n",
    "\n",
    "5. Domain Knowledge:\n",
    "\n",
    "   Consider domain knowledge and the context of wer problem. Sometimes, we may have prior information that suggests a reasonable range or specific values for lambda.\n",
    "\n",
    "It's important to note that the choice of the optimal lambda depends on the specific dataset and problem. What works best can vary significantly from one case to another. Cross-validation is often recommended because it provides an empirical, data-driven way to select the best lambda while avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617338a1",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2e9de21",
   "metadata": {},
   "source": [
    "\n",
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using cross-validation. Cross-validation is a technique for evaluating the performance of a model on a held-out dataset.\n",
    "\n",
    "To choose the optimal value of lambda using cross-validation, we can follow these steps:\n",
    "\n",
    "Split your dataset into a training set and a validation set.\n",
    "Train a Lasso Regression model on the training set using a range of lambda values.\n",
    "Evaluate the performance of each model on the validation set.\n",
    "Choose the lambda value that produces the best performance on the validation set.\n",
    "Here is an example of how to choose the optimal value of lambda using cross-validation:\n",
    "\n",
    "Split your dataset into 10 folds.\n",
    "For each fold:\n",
    "Train a Lasso Regression model on the remaining 9 folds using a range of lambda values.\n",
    "Evaluate the performance of the model on the held-out fold.\n",
    "Calculate the average performance of the model over all 10 folds for each lambda value.\n",
    "Choose the lambda value that produces the best average performance.\n",
    "Once you have chosen the optimal value of lambda, you can train a Lasso Regression model on the entire dataset using the chosen lambda value.\n",
    "\n",
    "Here are some additional tips for choosing the optimal value of lambda:\n",
    "\n",
    "Use a logarithmic grid of lambda values. This will allow you to explore a wider range of lambda values and find the optimal value more accurately.\n",
    "Use a large number of folds in the cross-validation procedure. This will give you a more reliable estimate of the model's performance.\n",
    "Consider using a nested cross-validation procedure. This is a more advanced cross-validation technique that can be used to tune multiple hyperparameters at the same time.\n",
    "Overall, cross-validation is a powerful technique for choosing the optimal value of the regularization parameter (lambda) in Lasso Regression. It can help you to improve the accuracy and interpretability of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff13e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
